So, what's going on here? Well, there's three steps to this process. First of all, the Map function receives independent lists of tokens from the source text. In parallel, it will attempt to strip off any leading or trailing punctuation, and if it is a well formed word, adds it to the results list with a count of 1. All such tokens are processed in this way, and duplicates are also appended to this list.

After the Map operation is complete, the program needs to organize all the processed data. This is where the partition 
function comes in. This function receives a huge list of lists; each constituent list was processed by an instance of Map and thus contains a sequence of (token, 1) pairs. This function organizes all the data into a dictionary, so that now each token key has a list of (token, 1) tuples as a value. 

The final step is in the Reduce function. It receives arbitrary key:value pairs from the dictionary built-in partition, which it uses to generate the final words count. In parallel, the list in each dictionary entry is collapsed, summed, and the final count for each token key is returned by each instance of Reduce. 

The final result is a list of (token, count) tuples, where count refers to the total count of the token over the entire document.

Let's get to the Python multiprocessing code, and the other stuff we need to make the above functions work. I'll create a worker pool of eight processes, and distribute the above workload to those processes via the Pool.map function.

